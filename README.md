Databricks AI 14-Day Challenge

This repository documents my structured 14-day hands-on learning challenge focused on Databricks, PySpark, and real-world analytics workflows.
The objective of this challenge is to build practical data engineering and business analytics skills using production-like datasets.








Databricks Day 4 – Delta Lake Introduction

Databricks 14 Days AI Challenge | #DatabricksWithIDC

This repository documents my hands-on work for Day 4 of the Databricks 14 Days AI Challenge, focused on understanding and implementing Delta Lake — the storage layer that brings reliability, consistency, and auditability to modern data lakes.

The objective of this exercise was to move beyond raw file-based analytics and implement production-ready data engineering practices that ensure clean, trustworthy data before it is used for reporting, dashboards, and business decision-making.


What This Project Covers

1. Converting Raw CSV to Delta Format

Loaded e-commerce CSV datasets into Spark DataFrames

Converted raw file-based storage into Delta tables for improved reliability and performance



2. Creating Delta Tables (SQL & PySpark)

Created Delta volumes and managed Delta tables using:

Spark SQL

PySpark DataFrame APIs

Established structured and queryable datasets for analytics workloads



3. Schema Enforcement & Data Protection

Implemented schema enforcement to prevent invalid or mismatched data from entering tables

Tested schema mismatch scenarios to understand real-world data quality failures



4. Handling Duplicate Inserts

Simulated duplicate and conflicting inserts

Implemented MERGE (UPSERT) operations to safely update existing records and insert only valid new records

Why This Matters for Business Analytics

Reliable analytics depends on reliable data foundations.


This project demonstrates how modern organizations:

Prevent incorrect data from corrupting dashboards

Maintain consistent schemas across evolving datasets

Control duplicate records to avoid inflated metrics

Build audit-ready data pipelines for reporting, forecasting, and decision-making

Tools & Technologies

Databricks

Apache Spark

PySpark

Spark SQL

Delta Lake

GitHub

This project reflects hands-on experience with enterprise data engineering foundations that directly support Business Analytics, Product Analytics, and Data Platform workflows.
